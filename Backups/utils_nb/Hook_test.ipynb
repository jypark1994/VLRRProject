{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597370662109",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HookWrapper(nn.Module):\n",
    "    '''\n",
    "        Wrapper class for Forward/Backward feature map extraction.\n",
    "\n",
    "        - Usage -\n",
    "        1) Make the instance of this class with the model and target layers.\n",
    "        2) Forward/Backward it.\n",
    "        3) Call get_features() will return the feature maps of previously forward/backwarded input.\n",
    "        4) Back to 2).\n",
    "    '''\n",
    "    def __init__(self, model, target_layers):\n",
    "        super(HookWrapper,self).__init__()\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.features = [] # Size will be 4 after hook\n",
    "\n",
    "        for name, module in model.named_children():\n",
    "            if name in target_layers:\n",
    "                module.register_forward_hook(self._extraction_fn)\n",
    "\n",
    "    def _extraction_fn(self, module, input, output):\n",
    "        # print(f\"Test : {output.shape}\")\n",
    "        self.features.append(output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_features(self): # Return feature list and make it empty.\n",
    "        tmp = self.features\n",
    "        self.features = []\n",
    "        return tmp\n",
    "\n",
    "# target_layers = ['conv1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
    "target_layers = ['conv1']\n",
    "net_t = models.resnet18(pretrained=True)\n",
    "net_s = models.resnet18(pretrained=True)\n",
    "hook_net_t = HookWrapper(net_t, target_layers)\n",
    "hook_net_s = HookWrapper(net_s, target_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 1000])\ntorch.Size([1, 64, 56, 56]) torch.Size([1, 64, 56, 56])\ntensor(8.6964, grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "x_HR = torch.randn((1,3,224,224))\n",
    "x_LR = torch.randn((1,3,112,112))\n",
    "y = torch.tensor([900])\n",
    "\n",
    "pred_t = hook_net_t(x_HR)\n",
    "pred_s = hook_net_s(x_LR)\n",
    "print(pred_s.shape)\n",
    "\n",
    "features_t = hook_net_t.get_features()\n",
    "features_s = hook_net_s.get_features()\n",
    "\n",
    "downsample = nn.MaxPool2d(kernel_size=2)\n",
    "criterion_a = nn.MSELoss()\n",
    "criterion_b = nn.CrossEntropyLoss()\n",
    "\n",
    "beta = 0.1\n",
    "attention_loss = 0\n",
    "\n",
    "for f_t, f_s in zip(features_t, features_s):\n",
    "    f_t = downsample(f_t) # Transfer function of f_t -> f_s domain.\n",
    "    if(f_t.shape != f_s.shape):\n",
    "        continue\n",
    "    print(f_t.shape, f_s.shape)\n",
    "\n",
    "    attention_loss += criterion_a(f_t, f_s) # Activation-based Attention Transfer (Zagoryuko et al.)\n",
    "    \n",
    "classification_loss = criterion_b(pred_s, y)\n",
    "total_loss = classification_loss + (beta/2)*attention_loss\n",
    "\n",
    "print(total_loss)"
   ]
  }
 ]
}