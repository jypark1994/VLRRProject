{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import time\n",
    "import lrResnet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (30,30)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiscaleDataset(Dataset):\n",
    "    def __init__(self, root, transform, LR_scale=2, HR_size=[224,224]):               \n",
    "        self.image_list = []\n",
    "        self.LR_scale = LR_scale\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.LR_transform = transforms.Resize(HR_size[0]//LR_scale)\n",
    "        self.ToTensor = transforms.ToTensor()\n",
    "        \n",
    "        class_dirs = os.listdir(root)\n",
    "        \n",
    "        for label, class_dir in enumerate(class_dirs):\n",
    "            image_dirs = os.listdir(os.path.join(root,class_dir))\n",
    "            for img_path in image_dirs:\n",
    "                self.image_list.append([label, os.path.join(root, class_dir, img_path)])\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        label = self.image_list[index][0]\n",
    "        img = Image.open(self.image_list[index][1])\n",
    "        \n",
    "        img_modified = self.transform(img)\n",
    "        \n",
    "        HR = self.ToTensor(img_modified)\n",
    "        LR = self.ToTensor(self.LR_transform(img_modified))\n",
    "        \n",
    "        # Exception handling for the grayscale images.\n",
    "        if(HR.shape[0] == 1):\n",
    "            HR = HR.repeat(3, 1, 1)\n",
    "        if(LR.shape[0] == 1):\n",
    "            LR = LR.repeat(3, 1, 1)\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        return LR, HR, label\n",
    "    \n",
    "def get_train_loader(root='/data/ILSVRC_Birds/train/', LR_scale=2, HR_size=[224,224], batch_size=128, num_workers=8):\n",
    "    transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = MultiscaleDataset(root, transform=transform, LR_scale=LR_scale, HR_size=HR_size)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return train_dataloader\n",
    "\n",
    "def get_test_loader(root='/data/ILSVRC_Birds/val/', LR_scale=2, HR_size=[224,224], batch_size=128, num_workers=8):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "    ])\n",
    "    \n",
    "    test_dataset = MultiscaleDataset(root, transform=transform, LR_scale=LR_scale, HR_size=HR_size)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    return test_dataloader\n",
    "\n",
    "LR_scale = 4\n",
    "train_loader = get_train_loader(LR_scale=LR_scale, batch_size=256, num_workers=8)\n",
    "test_loader = get_test_loader(LR_scale=LR_scale, batch_size=256, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     LR, HR, label = data[0], data[1], data[2]\n",
    "#     print(HR.shape, LR.shape, label.shape)\n",
    "    \n",
    "#     fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "#     LR_samples = make_grid(LR, nrow=8).permute(1,2,0)\n",
    "#     HR_samples = make_grid(HR, nrow=8).permute(1,2,0)\n",
    "#     ax[0].imshow(HR_samples)\n",
    "#     ax[0].axis('off')\n",
    "#     ax[0].set_title('HR Samples')\n",
    "#     ax[1].imshow(LR_samples)\n",
    "#     ax[1].axis('off')\n",
    "#     ax[1].set_title('LR Samples')\n",
    "#     plt.show()\n",
    "#     break\n",
    "    \n",
    "# for data in test_loader:\n",
    "#     LR, HR, label = data[0], data[1], data[2]\n",
    "#     print(HR.shape, LR.shape, label.shape)\n",
    "    \n",
    "#     fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "#     LR_samples = make_grid(LR, nrow=8).permute(1,2,0)\n",
    "#     HR_samples = make_grid(HR, nrow=8).permute(1,2,0)\n",
    "#     ax[0].imshow(HR_samples)\n",
    "#     ax[0].axis('off')\n",
    "#     ax[0].set_title('HR Samples')\n",
    "#     ax[1].imshow(LR_samples)\n",
    "#     ax[1].axis('off')\n",
    "#     ax[1].set_title('LR Samples')\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, down_scale=2, pretrained=False, num_classes = 18):\n",
    "    if model == 'resnet50':\n",
    "        if down_scale == 1:\n",
    "            net = models.resnet50(pretrained=pretrained)\n",
    "        else:\n",
    "            net = lrResnet.resnet50_LR(scale=down_scale, pretrained=pretrained)\n",
    "        net.fc = nn.Linear(in_features=2048, out_features=num_classes)\n",
    "    elif model == 'resnet34':\n",
    "        if down_scale == 1:\n",
    "            net = models.resnet34(pretrained=pretrained)\n",
    "        else:\n",
    "            net = lrResnet.resnet34_LR(scale=down_scale, pretrained=pretrained)\n",
    "        net.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
    "        \n",
    "    elif model == 'resnet18':\n",
    "        if down_scale == 1:\n",
    "            net = models.resnet18(pretrained=pretrained)\n",
    "        else:\n",
    "            net = lrResnet.resnet18_LR(scale=down_scale, pretrained=pretrained)\n",
    "        net.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HookWrapper(nn.Module):\n",
    "    '''\n",
    "        Wrapper class for Forward/Backward feature map extraction.\n",
    "\n",
    "        - Usage -\n",
    "        1) Make the instance of this class with the model and target layers.\n",
    "        2) Forward/Backward it.\n",
    "        3) Call get_features() will return the feature maps of previously forward/backwarded input.\n",
    "        4) Back to 2).\n",
    "    '''\n",
    "    def __init__(self, model, target_layers):\n",
    "        super(HookWrapper,self).__init__()\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.features = [] # Size will be 4 after hook\n",
    "\n",
    "        for name, module in model.named_children():\n",
    "            if name in target_layers:\n",
    "                module.register_forward_hook(self._extraction_fn)\n",
    "\n",
    "    def _extraction_fn(self, module, input, output):\n",
    "        # print(f\"Test : {output.shape}\")\n",
    "        self.features.append(output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_features(self): # Return feature list and make it empty.\n",
    "        tmp = self.features\n",
    "        self.features = []\n",
    "        return tmp\n",
    "\n",
    "target_layers = ['conv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Teacher: resnet18, Student: resnet18\nDistillate teacher's HR(x1) knowledge (92.67%) to the student.\n"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_name = \"birds\"\n",
    "\n",
    "teacher_name = 'resnet18'\n",
    "student_name = 'resnet18'\n",
    "print(f\"Teacher: {teacher_name}, Student: {student_name}\")\n",
    "\n",
    "net_t = get_model(teacher_name, 1, pretrained=True, num_classes=18)\n",
    "net_t = net_t.to(device)\n",
    "teacher_dict = torch.load(f\"./models/{data_name}/best_{teacher_name}_x1_True.pth\")\n",
    "\n",
    "net_t.load_state_dict(teacher_dict['net'])\n",
    "\n",
    "net_s = get_model(student_name, LR_scale, pretrained=False, num_classes=18)\n",
    "net_s = net_s.to(device)\n",
    "# student_dict = torch.load(f\"./models/birds/best_resnet18_x{down_scale}_True.pth\")\n",
    "\n",
    "print(f\"Distillate teacher's HR(x1) knowledge ({teacher_dict['acc']*100:.2f}%) to the student.\")\n",
    "# print(f\"Using pretrained student's LR(x2) knowledge ({student_dict['acc']*100:.2f}%) to the student.\")\n",
    "\n",
    "hook_net_t = HookWrapper(net_t, target_layers)\n",
    "hook_net_s = HookWrapper(net_s, target_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple GPU support\n",
    "hook_net_t = nn.DataParallel(hook_net_t)\n",
    "hook_net_s = nn.DataParallel(hook_net_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New loss function between LR and HR representation\n",
    "1. Fitting the feature map size \n",
    "    - Teacher(1x) : 224 -> (MaxPool) -> 112 -> 56 -> 28 -> 14 -> 7 GAP\n",
    "    - Student (Smaller ResNets)\n",
    "        - Ch  : 3   -> 64  -> 64  -> 128 -> 256 -> 512 \n",
    "        - 2x  : 112 -> 112 -> 56  -> 28  -> 14  -> 7   -> GAP\n",
    "        - 4x  : 56  -> 56  -> 28  -> 14  -> 7   -> 7   -> GAP\n",
    "        - 8x  : 28  -> 28  -> 14  -> 14  -> 7   -> 7   -> GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KD_loss():\n",
    "    def __init__(self, params):\n",
    "        self.fn = nn.KLDivLoss()\n",
    "        self.alpha = params['alpha']\n",
    "        self.T = params['T']\n",
    "    def __call__(self, student_outputs, teacher_outputs, label):\n",
    "        KD_loss = nn.KLDivLoss()(F.log_softmax(student_outputs/self.T, dim=1),\n",
    "                             F.softmax(teacher_outputs/self.T, dim=1)) * ((1-self.alpha) * self.T * self.T) + \\\n",
    "              F.cross_entropy(student_outputs, label) * (self.alpha)\n",
    "        return KD_loss\n",
    "\n",
    "\n",
    "def distillate(teacher, student, train_loader):\n",
    "    params = {'alpha':0.95, 'T':4, 'beta':0.1}\n",
    "\n",
    "    # According to 'Low-resolution visual recognition via deep feature distillation(DFD)',\n",
    "    # They use MSE Loss for distillation loss instead of NLL loss.\n",
    "\n",
    "    criterion = KD_loss(params)\n",
    "    AT_criterion = nn.MSELoss()\n",
    "    \n",
    "    # DFD uses SGD with momentum 0.9, weight deacy 5e-4.\n",
    "    optimizer = optim.SGD(student.parameters(),lr=1e-2, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.5)\n",
    "\n",
    "    attention_loss = 0\n",
    "    \n",
    "    train_avg_loss = 0\n",
    "    n_count = 0\n",
    "    n_corrects = 0\n",
    "\n",
    "    teacher.eval()\n",
    "    teacher.requires_grad = False\n",
    "    student.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        batch_lr, batch_hr, label = data[0].to(device), data[1].to(device), data[2].to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_t = teacher(batch_hr)\n",
    "        pred_s = student(batch_lr)\n",
    "\n",
    "        # Feature map extraction from both teacher and student networks.\n",
    "        # Current target : Conv1\n",
    "        feature_t = teacher.module.get_features()[0]\n",
    "        feature_s = student.module.get_features()[0]\n",
    "\n",
    "        # Since the input size of the maps are different in teacher/student,\n",
    "        # we downsample teacher's feature beforehand calculate attention loss.\n",
    "        # The kernel size can be varied according to the LR scale.\n",
    "        feature_t = nn.AvgPool2d(kernel_size=4)(feature_t)\n",
    "\n",
    "        loss = criterion(pred_s, pred_t, label) + params['beta']*AT_criterion(feature_t, feature_s)\n",
    "    \n",
    "        train_avg_loss += loss\n",
    "\n",
    "        n_corrects += torch.sum(torch.argmax(pred_s, dim=1) == label).item()\n",
    "        n_count += label.shape[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i == len(train_loader)-1):\n",
    "            print(f\"loss = {loss:.4f}\")\n",
    "\n",
    "    train_accuracy = n_corrects/n_count\n",
    "    train_avg_loss /= n_count\n",
    "\n",
    "    return train_accuracy, train_avg_loss, student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, test_loader, eval_target='LR'):\n",
    "    net.eval()\n",
    "    \n",
    "    n_count = 0\n",
    "    n_corrects = 0\n",
    "    \n",
    "    if eval_target=='LR':\n",
    "        target = 0\n",
    "    else:\n",
    "        target = 1\n",
    "\n",
    "    for j, data in enumerate(test_loader):\n",
    "        batch, label = data[target].to(device), data[2].to(device)\n",
    "\n",
    "        pred = net(batch)\n",
    "\n",
    "        n_corrects += torch.sum(torch.argmax(pred, dim=1) == label).item()\n",
    "        n_count += label.shape[0]\n",
    "\n",
    "    test_accuracy = n_corrects/n_count\n",
    "    \n",
    "    return test_accuracy, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "─── Start Training & Evalutation ───\n┌ Epoch (0/299)\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (18) must match the size of tensor b (28) at non-singleton dimension 3",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0afeaaebb835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_net_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_net_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"./models/distillate/T{teacher_name}_S{student_name}_x{LR_scale}.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m# ex : Tresnet34_Sresnet18_x2.pth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0afeaaebb835>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(teacher, student, epochs, train_loader, test_loader, save_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"┌ Epoch ({i}/{epochs-1})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistillate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"├── Training Loss : {loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'├── Training accuracy : {train_acc*100:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-724a7f8a6408>\u001b[0m in \u001b[0;36mdistillate\u001b[0;34m(teacher, student, train_loader)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfeature_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAvgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mAT_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtrain_avg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2536\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2538\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (18) must match the size of tensor b (28) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "def train_and_eval(teacher, student, epochs, train_loader, test_loader, save_name='default.pth'):\n",
    "    print(\"─── Start Training & Evalutation ───\")\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        time_start = time.time()\n",
    "        print(f\"┌ Epoch ({i}/{epochs-1})\")\n",
    "        \n",
    "        train_acc, loss, student = distillate(teacher, student, train_loader)\n",
    "        print(f\"├── Training Loss : {loss:.4f}\")\n",
    "        print(f'├── Training accuracy : {train_acc*100:.2f}%')\n",
    "        print(\"│ Testing ...\")\n",
    "        test_acc, student = evaluate(student, test_loader, eval_target='LR')\n",
    "        print(f'└── Testing accuracy : {test_acc*100:.2f}%')\n",
    "        \n",
    "        if test_acc > best_accuracy:\n",
    "            print(f\"  └──> Saving the best model to \\\"{save_name}\\\"\")\n",
    "            best_accuracy = test_acc\n",
    "            best_model = student\n",
    "            model_dict = {'acc':best_accuracy, 'net':best_model}\n",
    "            torch.save(model_dict, save_name)\n",
    "            \n",
    "        time_end = time.time()\n",
    "            \n",
    "        epoch_time = time_end - time_start\n",
    "        epoch_time_gm = time.gmtime(epoch_time)\n",
    "        estimated_time = epoch_time * (epochs - 1 - i)\n",
    "        estimated_time_gm = time.gmtime(estimated_time)\n",
    "        print(f\"Epoch time ─ {epoch_time_gm.tm_hour}[h] {epoch_time_gm.tm_min}[m] {epoch_time_gm.tm_sec}[s]\")\n",
    "        print(f\"Estimated time ─ {estimated_time_gm.tm_hour}[h] {estimated_time_gm.tm_min}[m] {estimated_time_gm.tm_sec}[s]\")   \n",
    "        print(\"\\n\")\n",
    "            \n",
    "    return best_accuracy, best_model\n",
    "        \n",
    "epochs = 300\n",
    "accuracy, net_t = train_and_eval(hook_net_t, hook_net_s, epochs, train_loader, test_loader, save_name=f\"./models/distillate/T{teacher_name}_S{student_name}_x{LR_scale}.pth\")\n",
    "# ex : Tresnet34_Sresnet18_x2.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "## 1. DataParallel efficiency\n",
    "- ResNet50(x1) -> ResNet18(x2) 기준 (img_sz=224, BS=64, epochs=150, num_workers=8)\n",
    "    - Target Volatile GPU utilization : 80~90%\n",
    "    - Single : 133 sec/epoch (5h 32m)\n",
    "    - Double : 106 sec/epoch (4h 24m)\n",
    "    - Quad : 92 sec/epoch (3h 50m)\n",
    "    - Quad+ (BS=128) : 62 sec/epoch (2h 36m)\n",
    "    - Quad++ (BS=256) : 52 sec/epoch (2h 9m)\n",
    "## 2. Classification Performance\n",
    "- ILSVRC Birds\n",
    "    - ResNet18(x1) ---Distillate--> ResNet18(x4)\n",
    "    - x1 에서 정확도 92.67%, x4 에서 87.22%\n",
    "    - 정확도 87.22% 보다 높게 나오면 성공 !\n",
    "    - MSE Loss vs KLdiv Loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1597372461532"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}